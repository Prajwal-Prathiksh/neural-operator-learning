{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5df5371f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training points: 100\n",
      "\n",
      "--- Results ---\n",
      "Input      True       Pred       RelError   Std (Unc)  95% CI\n",
      "1.0        14.1251    14.1285    2.4e-04    0.0038     [14.12, 14.14]\n",
      "2.0        33.0702    33.0731    8.7e-05    0.0028     [33.07, 33.08]\n",
      "3.0        66.3814    66.4215    6.0e-04    0.0041     [66.41, 66.43]\n",
      "4.0        120.6513   120.5740   6.4e-04    0.0027     [120.57, 120.58]\n",
      "5.0        201.0323   196.6150   2.2e-02    0.0415     [196.53, 196.70]\n",
      "6.0        312.8748   38.7002    8.8e-01    0.9655     [36.81, 40.59]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(x) + x**3 + x**2 + 10 * x + 1 + np.cos(5 * x)\n",
    "\n",
    "\n",
    "# 1. Setup Data\n",
    "np.random.seed(0)\n",
    "N_train = 100\n",
    "X = np.random.uniform(-10, 5, (N_train, 1))\n",
    "y = f(X).flatten()\n",
    "print(\"Number of training points:\", N_train)\n",
    "\n",
    "\n",
    "# Kernel Function (RBF)\n",
    "def kernel(x1, x2, l=0.5):\n",
    "    sq_dist = np.sum(x1**2, 1).reshape(-1, 1) + np.sum(x2**2, 1) - 2 * np.dot(x1, x2.T)\n",
    "    return np.exp(-0.5 / (l**2) * sq_dist)\n",
    "\n",
    "\n",
    "# 2. Training (Building K and alpha)\n",
    "K = kernel(X, X)\n",
    "\n",
    "# FIX: Add \"Jitter\" / Noise variance to the diagonal\n",
    "# This makes the matrix invertible\n",
    "sigma_n = 1e-5\n",
    "K[np.diag_indices_from(K)] += sigma_n\n",
    "alpha = np.linalg.solve(K, y)  # K^-1 * y\n",
    "\n",
    "# 3. Inference (N Test Points)\n",
    "# We define x_star with shape (N_test, D)\n",
    "x_star = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)\n",
    "k_star = kernel(X, x_star)  # Similarity matrix: Shape (N_train, N_test)\n",
    "\n",
    "# --- PREDICTION (Mean) ---\n",
    "# Shape: (N_test, N_train) @ (N_train,) -> (N_test,)\n",
    "mu_star = np.dot(k_star.T, alpha)\n",
    "\n",
    "# --- UNCERTAINTY (Variance) ---\n",
    "# 1. Prior Covariance (K_ss) between test points\n",
    "K_ss = kernel(x_star, x_star)  # Shape (N_test, N_test)\n",
    "\n",
    "# 2. Posterior Covariance: K_ss - K_*X * K_XX^-1 * K_X*\n",
    "# This results in a full covariance matrix (N_test, N_test)\n",
    "covariance_matrix = K_ss - np.dot(k_star.T, np.linalg.solve(K, k_star))\n",
    "\n",
    "# We only need the diagonal elements (variance at each point)\n",
    "uncertainty_variance = np.diag(covariance_matrix)\n",
    "uncertainty_std = np.sqrt(np.maximum(0, uncertainty_variance))\n",
    "\n",
    "# --- PRINTING ---\n",
    "true_values = f(x_star).flatten()\n",
    "\n",
    "print(\"\\n--- Results ---\")\n",
    "print(\n",
    "    f\"{'Input':<10} {'True':<10} {'Pred':<10} {'RelError':<10} {'Std (Unc)':<10} {'95% CI'}\"\n",
    ")\n",
    "for i in range(len(x_star)):\n",
    "    x_val = x_star[i, 0]\n",
    "    mu = mu_star[i]\n",
    "    std = uncertainty_std[i]\n",
    "    tv = true_values[i]\n",
    "    rel_error = np.abs(tv - mu) / (np.abs(tv) + 1e-8)\n",
    "\n",
    "    # 95% Confidence Interval is roughly +/- 1.96 * std\n",
    "    ci_lower = mu - 1.96 * std\n",
    "    ci_upper = mu + 1.96 * std\n",
    "\n",
    "    print(\n",
    "        f\"{x_val:<10.1f} {tv:<10.4f} {mu:<10.4f} {rel_error:<10.1e} {std:<10.4f} [{ci_lower:.2f}, {ci_upper:.2f}]\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-operator-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
